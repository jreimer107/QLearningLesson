{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "goal_reward = 1\n",
    "nongoal_reward = -0.2\n",
    "learning_rate =0.5\n",
    "discount = 0.9\n",
    "initial_q_value_range = (-2,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env():\n",
    "    def __init__(self):\n",
    "        self.height = 5\n",
    "        self.width = 5\n",
    "        self.posX = 0\n",
    "        self.posY = 0\n",
    "        self.endX = self.width-1\n",
    "        self.endY = self.height-1\n",
    "        self.actions = [0, 1, 2, 3]\n",
    "        self.stateCount = self.height*self.width\n",
    "        self.actionCount = len(self.actions)\n",
    "        self.step_count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.posX = 0\n",
    "        self.posY = 0\n",
    "        self.step_count = 0\n",
    "        self.done = False\n",
    "        return 0, 0, False\n",
    "\n",
    "    # take action\n",
    "    def step(self, action):\n",
    "        if action==0: # left\n",
    "            self.posX = self.posX-1 if self.posX>0 else self.posX\n",
    "        if action==1: # right\n",
    "            self.posX = self.posX+1 if self.posX<self.width-1 else self.posX\n",
    "        if action==2: # up\n",
    "            self.posY = self.posY-1 if self.posY>0 else self.posY\n",
    "        if action==3: # down\n",
    "            self.posY = self.posY+1 if self.posY<self.height-1 else self.posY\n",
    "\n",
    "        done = self.posX==self.endX and self.posY==self.endY;\n",
    "        # mapping (x,y) position to number between 0 and 5x5-1=24\n",
    "        nextState = self.width*self.posY + self.posX\n",
    "        reward = goal_reward if done else nongoal_reward\n",
    "        self.step_count += 1\n",
    "        return nextState, reward, done\n",
    "\n",
    "    # display environment\n",
    "    def render(self):\n",
    "        ret = \"\"\n",
    "        for i in range(self.height):\n",
    "            for j in range(self.width):\n",
    "                if self.posY==i and self.posX==j:\n",
    "                    ret += \"O\"\n",
    "                elif self.endY==i and self.endX==j:\n",
    "                    ret += \"T\"\n",
    "                else:\n",
    "                    ret += \".\"\n",
    "            ret += \"\\n\"\n",
    "        print(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch # 50 / 50\n",
      ".....\n",
      ".....\n",
      ".....\n",
      "....O\n",
      "....T\n",
      "\n",
      "[[-1.83077654 -1.95556002 -1.83084428 -1.13628939]\n",
      " [-1.50234559 -0.03561862 -0.69040615 -0.41889164]\n",
      " [-0.96329892 -0.59240175 -0.38480063 -1.37121019]\n",
      " [-1.59958144 -0.64050959 -1.31170459 -1.44067526]\n",
      " [-0.648666   -1.36943078 -1.94092155 -0.68779483]\n",
      " [-1.32325768 -1.99525826 -1.33128705 -1.04030156]\n",
      " [-0.96533384 -0.86231797 -1.00649656 -1.80688517]\n",
      " [-1.42071477 -1.6557604  -0.7278709  -1.770733  ]\n",
      " [-1.43897701 -1.00036976 -0.76442341 -1.02371161]\n",
      " [-0.79189807 -0.73406129 -1.6535863  -0.64318289]\n",
      " [-1.24186386 -1.43014671 -1.27119911 -0.9336623 ]\n",
      " [-0.8520989  -0.19414459 -0.79045847 -0.79556819]\n",
      " [-1.12173154  0.00650602 -1.14772462 -1.54075289]\n",
      " [-0.72320912  0.22945113 -0.75926177 -1.78595418]\n",
      " [-0.89649617 -0.67323759 -0.66538607  0.47716792]\n",
      " [-1.59049644 -1.88257267 -1.36626999 -0.81517869]\n",
      " [-1.53689714 -1.50045791 -0.37473013 -1.8238872 ]\n",
      " [-1.9523384  -0.5885081  -1.34644282 -1.29496173]\n",
      " [-1.68392684 -1.2993794  -0.24668434 -0.75120907]\n",
      " [-1.31500584 -1.48545244 -1.68162663  0.7524088 ]\n",
      " [-1.09085533 -0.68353149 -1.84909423 -1.77472221]\n",
      " [-1.01219245 -0.9729018  -0.53725713 -0.98353422]\n",
      " [-0.94625082 -1.11980112 -0.82108394 -1.53886477]\n",
      " [-1.7560394  -0.26072739 -0.8604096  -0.494766  ]\n",
      " [-1.3571107  -0.85942859 -0.27510133 -1.74239618]]\n",
      "Done in 12 steps\n"
     ]
    }
   ],
   "source": [
    "# create environment\n",
    "env = Env()\n",
    "\n",
    "# QTable : contains the Q-Values for every (state,action) pair\n",
    "qtable = np.random.uniform(low=initial_q_value_range[0], high=initial_q_value_range[1], size=(env.stateCount, env.actionCount))\n",
    "\n",
    "# training loop\n",
    "epochs = 50\n",
    "for i in range(epochs):\n",
    "    state, reward, done = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        clear_output(wait=True)\n",
    "        print(\"epoch #\", i+1, \"/\", epochs)\n",
    "        env.render()\n",
    "        print(qtable)\n",
    "        \n",
    "        time.sleep(0.05) #So that movement is visible\n",
    "        \n",
    "        # Choose an action to take\n",
    "        action = np.argmax(qtable[state])\n",
    "        current_q = qtable[state][action]\n",
    "\n",
    "        # take action\n",
    "        next_state, reward, done = env.step(action)\n",
    "\n",
    "        # update qtable value with update equation\n",
    "        qtable[state][action] = (1-learning_rate) * current_q + learning_rate * (reward + discount * np.max(qtable[next_state]))\n",
    "\n",
    "        # update state\n",
    "        state = next_state\n",
    "    \n",
    "    print(f\"Done in {env.step_count} steps\")\n",
    "    time.sleep(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
